{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f871d393e2c072",
   "metadata": {},
   "source": [
    "### Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T15:03:30.327388Z",
     "start_time": "2024-11-10T15:03:30.057782Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from ChestXRayDataset import ChestXRayDataset\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from tqdm import tqdm\n",
    "import torch_optimizer as optim_mod\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "401d32cc3ee88c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.469551Z",
     "start_time": "2024-11-10T14:51:16.383213Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/data_entries/miccai2023_nih-cxr-lt_labels_train.csv')\n",
    "df_val = pd.read_csv('../data/data_entries/miccai2023_nih-cxr-lt_labels_val.csv')\n",
    "df_test = pd.read_csv('../data/data_entries/miccai2023_nih-cxr-lt_labels_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e064940c88cd6388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.603077Z",
     "start_time": "2024-11-10T14:51:16.598772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((78506, 22), (12533, 22), (21081, 22))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ef722b6d5bddb",
   "metadata": {},
   "source": [
    "Some images exist in images folder, but not in miccai labels and opposite\n",
    "\n",
    "To fix this problem We create list of all image IDs in both train and test _images folder\n",
    "then ensure that only images withing this list are loaded to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b83bbd4dfc8674d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.704227Z",
     "start_time": "2024-11-10T14:51:16.700688Z"
    }
   },
   "outputs": [],
   "source": [
    "image_dir_train = '../data/train_images'\n",
    "image_dir_test = '../data/test_images'\n",
    "# Define function to get valid image ids that exist in the image directory\n",
    "def get_valid_image_ids(df, image_dir):\n",
    "    # Get the set of image IDs that exist in the image directory\n",
    "    image_files = set(os.listdir(image_dir))  # List of all files in the image directory\n",
    "    # Check if image id exists in the image directory\n",
    "    valid_ids = df[df['id'].isin(image_files)]['id']\n",
    "    return valid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23bdd75f36a256e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.744561Z",
     "start_time": "2024-11-10T14:51:16.716264Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get valid image ids for train and test datasets\n",
    "valid_train_ids = get_valid_image_ids(df_train, image_dir_train)\n",
    "valid_val_ids = get_valid_image_ids(df_val, image_dir_train)\n",
    "valid_test_ids = get_valid_image_ids(df_test, image_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6a294e2f064fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.764801Z",
     "start_time": "2024-11-10T14:51:16.755889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((78506, 22), (8018, 22), (21081, 22))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the DataFrames to include only the valid image IDs\n",
    "df_train_valid = df_train[df_train['id'].isin(valid_train_ids)]\n",
    "df_val_valid = df_val[df_val['id'].isin(valid_val_ids)]\n",
    "df_test_valid = df_test[df_test['id'].isin(valid_test_ids)]\n",
    "\n",
    "df_train_valid.shape, df_val_valid.shape, df_test_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7554c8d5ef5eca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.786894Z",
     "start_time": "2024-11-10T14:51:16.782623Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_val = pd.concat([df_train_valid, df_val_valid], ignore_index=True)\n",
    "assert df_train_val.shape[0] == df_train_valid.shape[0] + df_val_valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0686f621c6b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.844212Z",
     "start_time": "2024-11-10T14:51:16.839329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86524, 21)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val = df_train_val.copy()\n",
    "#df_train_val.drop(columns=['Pneumomediastinum', 'subj_id'], inplace=True)\n",
    "# Labels such Pneumomediastinum and Hernia rarely appears, but we keep them to generlize the model\n",
    "df_train_val.drop(columns=['subj_id'], inplace=True)\n",
    "df_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ca2ca48ff9f98f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.880223Z",
     "start_time": "2024-11-10T14:51:16.876337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21081, 21)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Do same for testing\n",
    "df_test_valid = df_test_valid.copy()\n",
    "df_test_valid.drop(columns=['subj_id'], inplace=True)\n",
    "df_test_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb271b83551522",
   "metadata": {},
   "source": [
    "### Step 2: Label encoding and create target column \n",
    "\n",
    "#### Step 2.1: Check for Inconsistent Rows\n",
    "\n",
    "First, we’ll check if any rows violate the condition: if No Finding is 1, then all other categories should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ffa7c6aa03915a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.915785Z",
     "start_time": "2024-11-10T14:51:16.911387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis, Hernia, Infiltration, Mass, Nodule, Pleural Thickening, Pneumonia, Pneumothorax, Pneumoperitoneum, Pneumomediastinum, Subcutaneous Emphysema, Tortuous Aorta, Calcification of the Aorta, No Finding]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# For train data\n",
    "inconsistent_rows = df_train_val[(df_train_val['No Finding'] == 1) & (df_train_val.iloc[:, 1:-1].sum(axis=1) > 0)]\n",
    "\n",
    "print(inconsistent_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6570a581d8f436",
   "metadata": {},
   "source": [
    "#### Step 2.2: create mappings variable for all categories we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89181548de9fd7d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.981160Z",
     "start_time": "2024-11-10T14:51:16.978297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Mapping:\n",
      "{'Atelectasis': 0, 'Cardiomegaly': 1, 'Consolidation': 2, 'Edema': 3, 'Effusion': 4, 'Emphysema': 5, 'Fibrosis': 6, 'Hernia': 7, 'Infiltration': 8, 'Mass': 9, 'Nodule': 10, 'Pleural Thickening': 11, 'Pneumonia': 12, 'Pneumothorax': 13, 'Pneumoperitoneum': 14, 'Pneumomediastinum': 15, 'Subcutaneous Emphysema': 16, 'Tortuous Aorta': 17, 'Calcification of the Aorta': 18, 'No Finding': 19}\n"
     ]
    }
   ],
   "source": [
    "# Extract all categories (exclude 'id')\n",
    "categories = df_train_val.columns[1:]\n",
    "\n",
    "# Create a mapping dictionary for categories to numbers\n",
    "category_mapping = {category: idx for idx, category in enumerate(categories)}\n",
    "\n",
    "print(\"Category Mapping:\")\n",
    "print(category_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed974c46c22bd9b",
   "metadata": {},
   "source": [
    "### Step 2.3: Create finding column\n",
    "\n",
    "This column will contain list of all finding categories for each image, and we have the following scenarios\n",
    "1. The image has no sickness ==> finding column is a list with only one item 'No Finding'\n",
    "2. The image contains only one category i.g 'Hernia' ==> finding column is a list with only one item 'Hernia'\n",
    "3. The image contains more than one category i.g 'Hernia' and 'Edema'...etc. ==> finding column is a list finding items 'Hernia' and 'Edema'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "107bc595a018ba1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:16.997198Z",
     "start_time": "2024-11-10T14:51:16.994166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create 'finding' and 'finding_encoded' as a string based on category values\n",
    "def create_finding(row):\n",
    "    # Check if 'No Finding' is 1, indicating no other categories are marked\n",
    "    if row['No Finding'] == 1:\n",
    "        return ['No Finding'], str(category_mapping['No Finding'])  # Return encoded as a string\n",
    "    \n",
    "    else:\n",
    "        # Generate lists of findings and their encoded values\n",
    "        findings = [category for category in categories if row[category] == 1]\n",
    "        encoded_findings = [str(category_mapping[category]) for category in findings]\n",
    "        \n",
    "        # Join encoded findings as a single string for stratification\n",
    "        return findings, ','.join(encoded_findings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4881a0ea80d8958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.280554Z",
     "start_time": "2024-11-10T14:51:17.012610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>finding</th>\n",
       "      <th>finding_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>[Cardiomegaly]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>[Cardiomegaly, Emphysema]</td>\n",
       "      <td>1,5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>[Cardiomegaly, Effusion]</td>\n",
       "      <td>1,4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>[No Finding]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000004_000.png</td>\n",
       "      <td>[Mass, Nodule]</td>\n",
       "      <td>9,10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86519</th>\n",
       "      <td>00030601_000.png</td>\n",
       "      <td>[Atelectasis]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86520</th>\n",
       "      <td>00030661_000.png</td>\n",
       "      <td>[Atelectasis]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86521</th>\n",
       "      <td>00030703_000.png</td>\n",
       "      <td>[Nodule]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86522</th>\n",
       "      <td>00030703_001.png</td>\n",
       "      <td>[Nodule]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86523</th>\n",
       "      <td>00030711_000.png</td>\n",
       "      <td>[Infiltration, Mass]</td>\n",
       "      <td>8,9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86524 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                    finding finding_encoded\n",
       "0      00000001_000.png             [Cardiomegaly]               1\n",
       "1      00000001_001.png  [Cardiomegaly, Emphysema]             1,5\n",
       "2      00000001_002.png   [Cardiomegaly, Effusion]             1,4\n",
       "3      00000002_000.png               [No Finding]              19\n",
       "4      00000004_000.png             [Mass, Nodule]            9,10\n",
       "...                 ...                        ...             ...\n",
       "86519  00030601_000.png              [Atelectasis]               0\n",
       "86520  00030661_000.png              [Atelectasis]               0\n",
       "86521  00030703_000.png                   [Nodule]              10\n",
       "86522  00030703_001.png                   [Nodule]              10\n",
       "86523  00030711_000.png       [Infiltration, Mass]             8,9\n",
       "\n",
       "[86524 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply function to create 'finding' and 'finding_encoded' columns in train_val data\n",
    "df_train_val[['finding', 'finding_encoded']] = df_train_val.apply(\n",
    "    lambda row: pd.Series(create_finding(row)), axis=1\n",
    ")\n",
    "\n",
    "df_train_val[['id', 'finding', 'finding_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "606b654b434d8b78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.366409Z",
     "start_time": "2024-11-10T14:51:17.313012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>finding</th>\n",
       "      <th>finding_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000013_000.png</td>\n",
       "      <td>[No Finding]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000013_001.png</td>\n",
       "      <td>[Emphysema, Pneumothorax, Subcutaneous Emphysema]</td>\n",
       "      <td>5,13,16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000013_002.png</td>\n",
       "      <td>[Emphysema, Pneumothorax, Subcutaneous Emphysema]</td>\n",
       "      <td>5,13,16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000013_003.png</td>\n",
       "      <td>[Pleural Thickening]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000013_004.png</td>\n",
       "      <td>[Effusion, Emphysema, Infiltration, Pneumothor...</td>\n",
       "      <td>4,5,8,13,16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21076</th>\n",
       "      <td>00030800_000.png</td>\n",
       "      <td>[No Finding]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>00030802_000.png</td>\n",
       "      <td>[No Finding]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21078</th>\n",
       "      <td>00030803_000.png</td>\n",
       "      <td>[No Finding]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21079</th>\n",
       "      <td>00030804_000.png</td>\n",
       "      <td>[No Finding]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21080</th>\n",
       "      <td>00030805_000.png</td>\n",
       "      <td>[No Finding]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21081 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                            finding  \\\n",
       "0      00000013_000.png                                       [No Finding]   \n",
       "1      00000013_001.png  [Emphysema, Pneumothorax, Subcutaneous Emphysema]   \n",
       "2      00000013_002.png  [Emphysema, Pneumothorax, Subcutaneous Emphysema]   \n",
       "3      00000013_003.png                               [Pleural Thickening]   \n",
       "4      00000013_004.png  [Effusion, Emphysema, Infiltration, Pneumothor...   \n",
       "...                 ...                                                ...   \n",
       "21076  00030800_000.png                                       [No Finding]   \n",
       "21077  00030802_000.png                                       [No Finding]   \n",
       "21078  00030803_000.png                                       [No Finding]   \n",
       "21079  00030804_000.png                                       [No Finding]   \n",
       "21080  00030805_000.png                                       [No Finding]   \n",
       "\n",
       "      finding_encoded  \n",
       "0                  19  \n",
       "1             5,13,16  \n",
       "2             5,13,16  \n",
       "3                  11  \n",
       "4         4,5,8,13,16  \n",
       "...               ...  \n",
       "21076              19  \n",
       "21077              19  \n",
       "21078              19  \n",
       "21079              19  \n",
       "21080              19  \n",
       "\n",
       "[21081 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply function to create 'finding' and 'finding_encoded' columns in test data\n",
    "df_test_valid[['finding', 'finding_encoded']] = df_test_valid.apply(\n",
    "    lambda row: pd.Series(create_finding(row)), axis=1\n",
    ")\n",
    "\n",
    "df_test_valid[['id', 'finding', 'finding_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "667e49a8-2cb9-4a70-9ee9-ee5ddd298d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            00000001_000.png00000001_001.png00000001_002.p...\n",
       "Atelectasis                                                                8280\n",
       "Cardiomegaly                                                               1707\n",
       "Consolidation                                                              2852\n",
       "Edema                                                                      1378\n",
       "Effusion                                                                   8659\n",
       "Emphysema                                                                  1423\n",
       "Fibrosis                                                                   1251\n",
       "Hernia                                                                      141\n",
       "Infiltration                                                              13782\n",
       "Mass                                                                       4034\n",
       "Nodule                                                                     4708\n",
       "Pleural Thickening                                                         2242\n",
       "Pneumonia                                                                   876\n",
       "Pneumothorax                                                               2637\n",
       "Pneumoperitoneum                                                            231\n",
       "Pneumomediastinum                                                            92\n",
       "Subcutaneous Emphysema                                                     1023\n",
       "Tortuous Aorta                                                              634\n",
       "Calcification of the Aorta                                                  395\n",
       "No Finding                                                                49746\n",
       "finding                       [Cardiomegaly, Cardiomegaly, Emphysema, Cardio...\n",
       "finding_encoded               11,51,4199,1019191919191984,8191911910,1884191...\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts_train = df_train_val.sum(axis=0)\n",
    "class_counts_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2316d09-047e-4d1f-9a0e-2d62cca6e4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                            00000013_000.png00000013_001.png00000013_002.p...\n",
       "Atelectasis                                                                2700\n",
       "Cardiomegaly                                                                868\n",
       "Consolidation                                                              1497\n",
       "Edema                                                                       751\n",
       "Effusion                                                                   3735\n",
       "Emphysema                                                                   917\n",
       "Fibrosis                                                                    365\n",
       "Hernia                                                                       62\n",
       "Infiltration                                                               5159\n",
       "Mass                                                                       1329\n",
       "Nodule                                                                     1305\n",
       "Pleural Thickening                                                          902\n",
       "Pneumonia                                                                   452\n",
       "Pneumothorax                                                               2106\n",
       "Pneumoperitoneum                                                             69\n",
       "Pneumomediastinum                                                           143\n",
       "Subcutaneous Emphysema                                                      813\n",
       "Tortuous Aorta                                                               95\n",
       "Calcification of the Aorta                                                   55\n",
       "No Finding                                                                 8015\n",
       "finding                       [No Finding, Emphysema, Pneumothorax, Subcutan...\n",
       "finding_encoded               195,13,165,13,16114,5,8,13,165,8,11,13,164,881...\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts_test = df_test_valid.sum(axis=0)\n",
    "class_counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b55b9-8e63-45cc-92b6-34583a9285fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8dfb5268ac40d9e",
   "metadata": {},
   "source": [
    "### Step 3: Create subsets and DataLoaders for the training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b2d115fab3676d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.386058Z",
     "start_time": "2024-11-10T14:51:17.383069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Image transformation for training and validation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1afaca69abb2753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.453755Z",
     "start_time": "2024-11-10T14:51:17.450933Z"
    }
   },
   "outputs": [],
   "source": [
    "subset_ratio = 0.99 # start with 30% of data\n",
    "train_val_ratio = 0.8 # For train, val ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eed4a8580d2f91",
   "metadata": {},
   "source": "#### Step 3.1 Take a Stratified 30% Subset Based on finding_encoded"
  },
  {
   "cell_type": "markdown",
   "id": "bf5139799d508b7f",
   "metadata": {},
   "source": [
    "**First: create the subset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8f38285699dcfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.558175Z",
     "start_time": "2024-11-10T14:51:17.493286Z"
    }
   },
   "outputs": [],
   "source": [
    "label_matrix = df_train_val['finding_encoded'].str.get_dummies(sep=',')\n",
    "\n",
    "# Initialize IterativeStratification\n",
    "stratifier = IterativeStratification(n_splits=2, order=1, sample_distribution_per_fold=[subset_ratio, 1 - subset_ratio]) # 1 - 0.3 = 0.7\n",
    "\n",
    "# Perform the stratified split\n",
    "train_indices, subset_indices = next(stratifier.split(df_train_val, label_matrix))\n",
    "\n",
    "# Create the subset and remaining dataframes\n",
    "subset_df_train_val = df_train_val.iloc[subset_indices].reset_index(drop=True)\n",
    "remaining_df_train_val = df_train_val.iloc[train_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3d1b5c6e52d2a37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.581218Z",
     "start_time": "2024-11-10T14:51:17.577475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial size of df_train_val: 86524 rows\n",
      "Subset size for training (80%): 85643 rows\n",
      "Remaining size not used (20%): 881 rows\n"
     ]
    }
   ],
   "source": [
    "# After the first stratified split (Subset and Remaining)\n",
    "initial_size = len(df_train_val)\n",
    "print(f\"Initial size of df_train_val: {initial_size} rows\")\n",
    "\n",
    "subset_size = len(subset_df_train_val)\n",
    "remaining_size = len(remaining_df_train_val)\n",
    "print(f\"Subset size for training (80%): {subset_size} rows\")\n",
    "print(f\"Remaining size not used (20%): {remaining_size} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1afcc2631c3ca09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.599817Z",
     "start_time": "2024-11-10T14:51:17.589223Z"
    }
   },
   "outputs": [],
   "source": [
    "assert initial_size == subset_size + remaining_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1d3031b-6358-44e3-a30b-5ff51c7c100c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85643, 23)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df_train_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f527ed90428ef0a",
   "metadata": {},
   "source": [
    "**Then: split to train, val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a732ae93903a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.658671Z",
     "start_time": "2024-11-10T14:51:17.645844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (80% of subset): 68442 rows\n",
      "Val size (20% of subset): 17201 rows\n"
     ]
    }
   ],
   "source": [
    "# Perform the stratified split\n",
    "label_matrix_2 = subset_df_train_val['finding_encoded'].str.get_dummies(sep=',')\n",
    "stratifier_2 = IterativeStratification(n_splits=2, order=1, sample_distribution_per_fold=[train_val_ratio, 1 - train_val_ratio]) \n",
    "val_indices, train_indices = next(stratifier_2.split(subset_df_train_val, label_matrix_2))\n",
    "\n",
    "train_df = subset_df_train_val.iloc[train_indices].reset_index(drop=True)\n",
    "val_df = subset_df_train_val.iloc[val_indices].reset_index(drop=True)\n",
    "\n",
    "# Verify the sizes\n",
    "print(f\"Train size (80% of subset): {len(train_df)} rows\")\n",
    "print(f\"Val size (20% of subset): {len(val_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac335e9f-7b24-425a-89c7-2266ab9dd441",
   "metadata": {},
   "source": [
    "#### Step 3.2: Create dataset objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e40dc70503bcdbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.695332Z",
     "start_time": "2024-11-10T14:51:17.691099Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = ChestXRayDataset(dataframe=train_df, image_dir=image_dir_train, category_mapping=category_mapping, transform=train_transform)\n",
    "\n",
    "val_dataset = ChestXRayDataset(dataframe=val_df, image_dir=image_dir_train, category_mapping=category_mapping, transform=val_transform)\n",
    "\n",
    "test_dataset = ChestXRayDataset(dataframe=df_test_valid, image_dir=image_dir_test, category_mapping=category_mapping, transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863c9c6b857131b",
   "metadata": {},
   "source": [
    "#### Step 3.3: Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df5bf589c2bf6208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.739788Z",
     "start_time": "2024-11-10T14:51:17.736347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba1c0b0048dc8cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.769415Z",
     "start_time": "2024-11-10T14:51:17.766407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataLoader: 2139 batches\n",
      "Validation DataLoader: 538 batches\n",
      "Testing DataLoader: 659 batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"Validation DataLoader: {len(val_loader)} batches\")\n",
    "print(f\"Testing DataLoader: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d38f82-08a5-432e-857c-f95ea6494ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9d374bdb477980c",
   "metadata": {},
   "source": [
    "### Step 4: Define the model\n",
    "We use pretrained ResNet\n",
    "- We add linear layer to predict multi-label ( more than one category per image)\n",
    "- ResNet-50 designed to output 2048 features\n",
    "- We add two Linear layer to avoid going down from 2048 features to only 20 which may cause instability and overfitting\n",
    "\n",
    "**Note**: Different additional params/functionality to the network i.g Dropout, normalization...etc. have been tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd0ae5b8bdfccf20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:17.892893Z",
     "start_time": "2024-11-10T14:51:17.801419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Number of unique categories (labels) in the 'finding_encoded' column\n",
    "num_classes = len(df_train_val['finding_encoded'].str.get_dummies(sep=',').columns)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a937808d-63bc-4296-98d3-9bede88bd856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=20, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet50(weights='IMAGENET1K_V2') \n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),  # to reduce complexity gradually (2048, 512)\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, 256), \n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes),  # Exclude 'id', 20 different categories\n",
    "    nn.Sigmoid()  # Sigmoid for multi-label classification\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5888561ce6ee3",
   "metadata": {},
   "source": [
    "### Step 5: Train, Validate and test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c917d6089b26bca",
   "metadata": {},
   "source": [
    "[Referance about BCEWithLogitsLoss()](https://medium.com/@sahilcarterr/why-nn-bcewithlogitsloss-numerically-stable-6a04f3052967)\n",
    "\n",
    "nn.BCEWithLogitsLoss():\n",
    "\n",
    "1. This loss function is more efficient because it combines the sigmoid activation and binary cross-entropy loss into a single function.\n",
    "2. It expects the raw logits (not passed through sigmoid) as input and applies the sigmoid internally.\n",
    "3. It's numerically more stable and faster than using nn.BCELoss() with a separate sigmoid.\n",
    "\n",
    "nn.BCELoss():\n",
    "\n",
    "1. This loss function expects the model's output to be probabilities in the range [0, 1], so it requires you to apply a sigmoid activation to the model's output beforehand.\n",
    "2. The formula for binary cross-entropy is applied after transforming the raw logits into probabilities using the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3224a10-8a9b-46c7-a6dc-efae37187e01",
   "metadata": {},
   "source": [
    "***Define weighted classes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9e96f0a-d427-4a64-af7e-9524f41c7c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finding_encoded\n",
       "0      6558\n",
       "1      1352\n",
       "2      2258\n",
       "3      1091\n",
       "4      6858\n",
       "5      1127\n",
       "6       986\n",
       "7       112\n",
       "8     10915\n",
       "9      3195\n",
       "10     3729\n",
       "11     1776\n",
       "12      694\n",
       "13     2089\n",
       "14      183\n",
       "15       73\n",
       "16      810\n",
       "17      502\n",
       "18      312\n",
       "19    39399\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories  = train_df['finding_encoded'].str.split(',').explode().value_counts()\n",
    "categories.index = categories.index.astype(int)\n",
    "categories = categories.sort_index()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c14fc83c-6561-4ed7-a9cb-7809d78ce2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68442"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = len(train_df)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d55a88c7-0fae-44db-90c0-ab2bc3e2585f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finding_encoded\n",
       "0     0.095818\n",
       "1     0.019754\n",
       "2     0.032991\n",
       "3     0.015941\n",
       "4     0.100202\n",
       "5     0.016466\n",
       "6     0.014406\n",
       "7     0.001636\n",
       "8     0.159478\n",
       "9     0.046682\n",
       "10    0.054484\n",
       "11    0.025949\n",
       "12    0.010140\n",
       "13    0.030522\n",
       "14    0.002674\n",
       "15    0.001067\n",
       "16    0.011835\n",
       "17    0.007335\n",
       "18    0.004559\n",
       "19    0.575655\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportions =  categories / total\n",
    "proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "442e7257-b678-4455-a8c0-415f2302742a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finding_encoded\n",
       "0      10.436414\n",
       "1      50.622781\n",
       "2      30.310895\n",
       "3      62.733272\n",
       "4       9.979878\n",
       "5      60.729370\n",
       "6      69.413793\n",
       "7     611.089286\n",
       "8       6.270454\n",
       "9      21.421596\n",
       "10     18.353982\n",
       "11     38.537162\n",
       "12     98.619597\n",
       "13     32.763045\n",
       "14    374.000000\n",
       "15    937.561644\n",
       "16     84.496296\n",
       "17    136.338645\n",
       "18    219.365385\n",
       "19      1.737151\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 1 / proportions\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3b2a6c1-fac3-4ec1-94e6-d3232b122c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finding_encoded\n",
       "0     0.003630\n",
       "1     0.017609\n",
       "2     0.010544\n",
       "3     0.021822\n",
       "4     0.003472\n",
       "5     0.021125\n",
       "6     0.024146\n",
       "7     0.212569\n",
       "8     0.002181\n",
       "9     0.007452\n",
       "10    0.006384\n",
       "11    0.013405\n",
       "12    0.034305\n",
       "13    0.011397\n",
       "14    0.130097\n",
       "15    0.326133\n",
       "16    0.029392\n",
       "17    0.047426\n",
       "18    0.076307\n",
       "19    0.000604\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_weights = weights / weights.sum()\n",
    "normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6df1375-d117-4e36-a20a-8a73084cf8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10.4364,  50.6228,  30.3109,  62.7333,   9.9799,  60.7294,  69.4138,\n",
       "        611.0893,   6.2705,  21.4216,  18.3540,  38.5372,  98.6196,  32.7630,\n",
       "        374.0000, 937.5616,  84.4963, 136.3386, 219.3654,   1.7372],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_tensor = torch.tensor(weights.values, dtype=torch.float32).to(device)\n",
    "weights_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67cf52d58324dd99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:18.265595Z",
     "start_time": "2024-11-10T14:51:18.263226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use BCEWithLogitsLoss for multi-label classification\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weights_tensor)   # Combined Sigmoid + Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19d9bd214485c5db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:51:18.295497Z",
     "start_time": "2024-11-10T14:51:18.292609Z"
    }
   },
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09d8f421-6cdf-4099-8c2d-711b2d36e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "lookahead_optimizer = optim_mod.Lookahead(optimizer, k=5, alpha=0.5)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e186b7feb8fd449b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:19:46.917355Z",
     "start_time": "2024-11-10T16:19:46.913493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store loss and accuracy per epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1d71fdcda40f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:20:56.440991Z",
     "start_time": "2024-11-10T16:20:56.436097Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels, _ in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        all_preds.append(torch.sigmoid(outputs).detach().cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    \n",
    "    # Print each label, prediction, and corresponding pos_weight (for debug and ensure everything as excepted)\n",
    "        #preds = torch.sigmoid(outputs).detach().cpu()\n",
    "        #for i in range(len(labels)):  # Loop through each image in the batch\n",
    "            #label = labels[i].cpu().numpy()\n",
    "            #pred = preds[i].numpy()\n",
    "            #print(f\"Label: {label}, Prediction: {pred}, Pos_Weight: {weights_tensor.cpu().numpy()}\")\n",
    "\n",
    "    # Calculate F1 scores and thresholds for each class\n",
    "    best_thresholds = []\n",
    "    for i in range(all_labels.shape[1]):\n",
    "        precision, recall, thresholds = precision_recall_curve(all_labels[:, i], all_preds[:, i])\n",
    "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        best_thresholds.append(best_threshold)\n",
    "\n",
    "    # Apply class-specific thresholds\n",
    "    thresholded_preds = np.column_stack([(all_preds[:, i] > best_thresholds[i]).astype(float) for i in range(all_labels.shape[1])])\n",
    "\n",
    "    correct_predictions = (thresholded_preds == all_labels).all(axis=1).sum()\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_accuracy = (correct_predictions / all_labels.shape[0]) * 100\n",
    "\n",
    "    report = classification_report(all_labels, thresholded_preds, zero_division=0, output_dict=True)\n",
    "    precision, recall, f1 = report['macro avg']['precision'], report['macro avg']['recall'], report['macro avg']['f1-score']\n",
    "\n",
    "    return avg_loss, avg_accuracy, precision, recall, f1, best_thresholds, correct_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfe66ca8682a5d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:21:01.873748Z",
     "start_time": "2024-11-10T16:21:01.868748Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, best_thresholds):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(dataloader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            all_preds.append(torch.sigmoid(outputs).cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    # Apply class-specific thresholds\n",
    "    thresholded_preds = np.column_stack([(all_preds[:, i] > best_thresholds[i]).astype(float) for i in range(all_labels.shape[1])])\n",
    "\n",
    "    correct_predictions = (thresholded_preds == all_labels).all(axis=1).sum()\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_accuracy = (correct_predictions / all_labels.shape[0]) * 100\n",
    "\n",
    "    report = classification_report(all_labels, thresholded_preds, zero_division=0, output_dict=True)\n",
    "    precision, recall, f1 = report['macro avg']['precision'], report['macro avg']['recall'], report['macro avg']['f1-score']\n",
    "\n",
    "    return avg_loss, avg_accuracy, precision, recall, f1, correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10426519-66bd-4eaf-a044-d0326ccace57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, criterion, device, label_names, best_thresholds):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(dataloader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            all_preds.append(torch.sigmoid(outputs).cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    # Dynamic threshold for each label based on train data\n",
    "    thresholded_preds = np.column_stack([(all_preds[:, i] > best_thresholds[i]).astype(float) for i in range(all_labels.shape[1])])\n",
    "\n",
    "    print(\"\\nFinding Best Thresholds for Each Class (Reported Only):\")\n",
    "    for i, threshold in enumerate(best_thresholds):\n",
    "        print(f\"Class '{label_names[i]}': Best Threshold = {threshold:.4f}\")\n",
    "\n",
    "    default_preds = (all_preds > 0.5).astype(float)\n",
    "\n",
    "    print(\"\\nClassification Report with Best Thresholds:\")\n",
    "    report_best = classification_report(all_labels, thresholded_preds, target_names=label_names, zero_division=0)\n",
    "    print(report_best)\n",
    "\n",
    "    print(\"\\nClassification Report with Default Threshold (0.5):\")\n",
    "    report_default = classification_report(all_labels, default_preds, target_names=label_names, zero_division=0)\n",
    "    print(report_default)\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return avg_loss, report_best, report_default, best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c94a5ce0-1b90-4c7c-ae11-628973243d3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:52:46.171949Z",
     "start_time": "2024-11-10T14:51:18.452045Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 7 # wait \n",
    "best_val = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# for plotting\n",
    "num_epochs_runned = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f35e36e-0752-4172-b86f-a6a713b9b3ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:52:46.171949Z",
     "start_time": "2024-11-10T14:51:18.452045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3262, Train Accuracy: 44.1352, Train Precision: 0.0758, Train Recall: 0.0912, Train F1: 0.0683, Train Best Threshold: 0.6660, Train correct predictions: 30207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 7530  all labels size: (17201, 20)\n",
      "Val Loss: 1.3208, Val Accuracy: 43.7765, Val Precision: 0.0789, Val Recall: 0.1115, Val F1: 0.0691, Val correct predictions: 7530\n",
      "Epoch 1: Learning rate = 0.001\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [2/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:14<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3253, Train Accuracy: 43.3900, Train Precision: 0.0710, Train Recall: 0.0901, Train F1: 0.0649, Train Best Threshold: 0.6723, Train correct predictions: 29697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6623  all labels size: (17201, 20)\n",
      "Val Loss: 1.3234, Val Accuracy: 38.5036, Val Precision: 0.0611, Val Recall: 0.1399, Val F1: 0.0729, Val correct predictions: 6623\n",
      "Epoch 2: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [3/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3187, Train Accuracy: 39.9082, Train Precision: 0.0822, Train Recall: 0.0957, Train F1: 0.0657, Train Best Threshold: 0.6991, Train correct predictions: 27314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 7270  all labels size: (17201, 20)\n",
      "Val Loss: 1.3160, Val Accuracy: 42.2650, Val Precision: 0.0743, Val Recall: 0.1136, Val F1: 0.0680, Val correct predictions: 7270\n",
      "Epoch 3: Learning rate = 0.001\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [4/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3131, Train Accuracy: 38.9761, Train Precision: 0.0823, Train Recall: 0.1021, Train F1: 0.0738, Train Best Threshold: 0.7088, Train correct predictions: 26676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 8215  all labels size: (17201, 20)\n",
      "Val Loss: 1.3205, Val Accuracy: 47.7589, Val Precision: 0.0629, Val Recall: 0.0816, Val F1: 0.0618, Val correct predictions: 8215\n",
      "Epoch 4: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [5/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3137, Train Accuracy: 40.7980, Train Precision: 0.0860, Train Recall: 0.1071, Train F1: 0.0769, Train Best Threshold: 0.7060, Train correct predictions: 27923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 8610  all labels size: (17201, 20)\n",
      "Val Loss: 1.3209, Val Accuracy: 50.0552, Val Precision: 0.0906, Val Recall: 0.0865, Val F1: 0.0700, Val correct predictions: 8610\n",
      "Epoch 5: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [6/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:17<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3106, Train Accuracy: 39.4728, Train Precision: 0.0926, Train Recall: 0.1113, Train F1: 0.0757, Train Best Threshold: 0.7106, Train correct predictions: 27016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5819  all labels size: (17201, 20)\n",
      "Val Loss: 1.3192, Val Accuracy: 33.8294, Val Precision: 0.0686, Val Recall: 0.1208, Val F1: 0.0692, Val correct predictions: 5819\n",
      "Epoch 6: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [7/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:17<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3132, Train Accuracy: 41.7580, Train Precision: 0.0849, Train Recall: 0.1307, Train F1: 0.0773, Train Best Threshold: 0.6858, Train correct predictions: 28580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5130  all labels size: (17201, 20)\n",
      "Val Loss: 1.3299, Val Accuracy: 29.8238, Val Precision: 0.0766, Val Recall: 0.1673, Val F1: 0.0827, Val correct predictions: 5130\n",
      "Epoch 7: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [8/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3078, Train Accuracy: 37.5617, Train Precision: 0.0910, Train Recall: 0.1297, Train F1: 0.0820, Train Best Threshold: 0.6974, Train correct predictions: 25708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:22<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 8622  all labels size: (17201, 20)\n",
      "Val Loss: 1.3215, Val Accuracy: 50.1250, Val Precision: 0.0752, Val Recall: 0.0926, Val F1: 0.0619, Val correct predictions: 8622\n",
      "Epoch 8: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [9/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3051, Train Accuracy: 39.0038, Train Precision: 0.0948, Train Recall: 0.1372, Train F1: 0.0830, Train Best Threshold: 0.7061, Train correct predictions: 26695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6412  all labels size: (17201, 20)\n",
      "Val Loss: 1.3103, Val Accuracy: 37.2769, Val Precision: 0.0977, Val Recall: 0.1529, Val F1: 0.0821, Val correct predictions: 6412\n",
      "Epoch 9: Learning rate = 0.001\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [10/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3027, Train Accuracy: 37.0825, Train Precision: 0.0982, Train Recall: 0.1327, Train F1: 0.0843, Train Best Threshold: 0.7128, Train correct predictions: 25380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6985  all labels size: (17201, 20)\n",
      "Val Loss: 1.3051, Val Accuracy: 40.6081, Val Precision: 0.0931, Val Recall: 0.1490, Val F1: 0.0855, Val correct predictions: 6985\n",
      "Epoch 10: Learning rate = 0.001\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [11/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2946, Train Accuracy: 35.3935, Train Precision: 0.1011, Train Recall: 0.1309, Train F1: 0.0782, Train Best Threshold: 0.7209, Train correct predictions: 24224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6889  all labels size: (17201, 20)\n",
      "Val Loss: 1.2977, Val Accuracy: 40.0500, Val Precision: 0.1106, Val Recall: 0.1238, Val F1: 0.0785, Val correct predictions: 6889\n",
      "Epoch 11: Learning rate = 0.001\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [12/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:17<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3142, Train Accuracy: 37.0752, Train Precision: 0.0869, Train Recall: 0.1193, Train F1: 0.0778, Train Best Threshold: 0.6966, Train correct predictions: 25375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6883  all labels size: (17201, 20)\n",
      "Val Loss: 1.3151, Val Accuracy: 40.0151, Val Precision: 0.0681, Val Recall: 0.0922, Val F1: 0.0577, Val correct predictions: 6883\n",
      "Epoch 12: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [13/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3071, Train Accuracy: 33.5817, Train Precision: 0.0910, Train Recall: 0.1376, Train F1: 0.0817, Train Best Threshold: 0.6986, Train correct predictions: 22984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 4727  all labels size: (17201, 20)\n",
      "Val Loss: 1.3066, Val Accuracy: 27.4810, Val Precision: 0.0836, Val Recall: 0.1401, Val F1: 0.0814, Val correct predictions: 4727\n",
      "Epoch 13: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [14/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3050, Train Accuracy: 34.0624, Train Precision: 0.0972, Train Recall: 0.1224, Train F1: 0.0810, Train Best Threshold: 0.7118, Train correct predictions: 23313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 4258  all labels size: (17201, 20)\n",
      "Val Loss: 1.3048, Val Accuracy: 24.7544, Val Precision: 0.0945, Val Recall: 0.1518, Val F1: 0.0738, Val correct predictions: 4258\n",
      "Epoch 14: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [15/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:17<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3011, Train Accuracy: 33.1814, Train Precision: 0.0983, Train Recall: 0.1231, Train F1: 0.0810, Train Best Threshold: 0.7174, Train correct predictions: 22710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 4962  all labels size: (17201, 20)\n",
      "Val Loss: 1.3011, Val Accuracy: 28.8472, Val Precision: 0.0933, Val Recall: 0.1311, Val F1: 0.0793, Val correct predictions: 4962\n",
      "Epoch 15: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [16/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3000, Train Accuracy: 33.4473, Train Precision: 0.0979, Train Recall: 0.1287, Train F1: 0.0817, Train Best Threshold: 0.7161, Train correct predictions: 22892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5200  all labels size: (17201, 20)\n",
      "Val Loss: 1.3042, Val Accuracy: 30.2308, Val Precision: 0.0805, Val Recall: 0.1156, Val F1: 0.0627, Val correct predictions: 5200\n",
      "Epoch 16: Learning rate = 0.001\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [17/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3039, Train Accuracy: 35.4417, Train Precision: 0.0947, Train Recall: 0.1333, Train F1: 0.0781, Train Best Threshold: 0.7071, Train correct predictions: 24257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 7432  all labels size: (17201, 20)\n",
      "Val Loss: 1.3020, Val Accuracy: 43.2068, Val Precision: 0.0918, Val Recall: 0.1182, Val F1: 0.0776, Val correct predictions: 7432\n",
      "Epoch 17: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [18/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2956, Train Accuracy: 32.7518, Train Precision: 0.1046, Train Recall: 0.1289, Train F1: 0.0759, Train Best Threshold: 0.7172, Train correct predictions: 22416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6228  all labels size: (17201, 20)\n",
      "Val Loss: 1.2965, Val Accuracy: 36.2072, Val Precision: 0.0810, Val Recall: 0.1176, Val F1: 0.0690, Val correct predictions: 6228\n",
      "Epoch 18: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [19/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:14<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2940, Train Accuracy: 33.0747, Train Precision: 0.0926, Train Recall: 0.1134, Train F1: 0.0706, Train Best Threshold: 0.7225, Train correct predictions: 22637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:22<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6543  all labels size: (17201, 20)\n",
      "Val Loss: 1.2997, Val Accuracy: 38.0385, Val Precision: 0.0866, Val Recall: 0.0948, Val F1: 0.0657, Val correct predictions: 6543\n",
      "Epoch 19: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [20/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:18<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2919, Train Accuracy: 33.8549, Train Precision: 0.1028, Train Recall: 0.1401, Train F1: 0.0822, Train Best Threshold: 0.7186, Train correct predictions: 23171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6750  all labels size: (17201, 20)\n",
      "Val Loss: 1.3036, Val Accuracy: 39.2419, Val Precision: 0.0948, Val Recall: 0.1073, Val F1: 0.0645, Val correct predictions: 6750\n",
      "Epoch 20: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [21/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2917, Train Accuracy: 34.8631, Train Precision: 0.1025, Train Recall: 0.1323, Train F1: 0.0775, Train Best Threshold: 0.7204, Train correct predictions: 23861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 4993  all labels size: (17201, 20)\n",
      "Val Loss: 1.2968, Val Accuracy: 29.0274, Val Precision: 0.0994, Val Recall: 0.1527, Val F1: 0.0818, Val correct predictions: 4993\n",
      "Epoch 21: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [22/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2889, Train Accuracy: 33.9411, Train Precision: 0.1014, Train Recall: 0.1443, Train F1: 0.0804, Train Best Threshold: 0.7184, Train correct predictions: 23230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6229  all labels size: (17201, 20)\n",
      "Val Loss: 1.2941, Val Accuracy: 36.2130, Val Precision: 0.0918, Val Recall: 0.1342, Val F1: 0.0717, Val correct predictions: 6229\n",
      "Epoch 22: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [23/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:14<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2875, Train Accuracy: 33.6650, Train Precision: 0.1057, Train Recall: 0.1393, Train F1: 0.0808, Train Best Threshold: 0.7224, Train correct predictions: 23041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 7159  all labels size: (17201, 20)\n",
      "Val Loss: 1.2937, Val Accuracy: 41.6197, Val Precision: 0.0950, Val Recall: 0.1118, Val F1: 0.0760, Val correct predictions: 7159\n",
      "Epoch 23: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [24/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2882, Train Accuracy: 34.3532, Train Precision: 0.1054, Train Recall: 0.1341, Train F1: 0.0792, Train Best Threshold: 0.7231, Train correct predictions: 23512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5478  all labels size: (17201, 20)\n",
      "Val Loss: 1.2940, Val Accuracy: 31.8470, Val Precision: 0.1009, Val Recall: 0.1524, Val F1: 0.0812, Val correct predictions: 5478\n",
      "Epoch 24: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [25/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:17<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2877, Train Accuracy: 33.3699, Train Precision: 0.1049, Train Recall: 0.1414, Train F1: 0.0821, Train Best Threshold: 0.7235, Train correct predictions: 22839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6085  all labels size: (17201, 20)\n",
      "Val Loss: 1.2900, Val Accuracy: 35.3759, Val Precision: 0.0899, Val Recall: 0.1426, Val F1: 0.0756, Val correct predictions: 6085\n",
      "Epoch 25: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [26/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2873, Train Accuracy: 32.4771, Train Precision: 0.1066, Train Recall: 0.1426, Train F1: 0.0756, Train Best Threshold: 0.7239, Train correct predictions: 22228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6378  all labels size: (17201, 20)\n",
      "Val Loss: 1.2852, Val Accuracy: 37.0792, Val Precision: 0.0938, Val Recall: 0.1554, Val F1: 0.0716, Val correct predictions: 6378\n",
      "Epoch 26: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [27/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2826, Train Accuracy: 33.6460, Train Precision: 0.1121, Train Recall: 0.1441, Train F1: 0.0792, Train Best Threshold: 0.7247, Train correct predictions: 23028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 3809  all labels size: (17201, 20)\n",
      "Val Loss: 1.2881, Val Accuracy: 22.1441, Val Precision: 0.1476, Val Recall: 0.1492, Val F1: 0.0658, Val correct predictions: 3809\n",
      "Epoch 27: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [28/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2809, Train Accuracy: 33.6051, Train Precision: 0.1076, Train Recall: 0.1708, Train F1: 0.0854, Train Best Threshold: 0.7210, Train correct predictions: 23000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5133  all labels size: (17201, 20)\n",
      "Val Loss: 1.2844, Val Accuracy: 29.8413, Val Precision: 0.1023, Val Recall: 0.1930, Val F1: 0.0895, Val correct predictions: 5133\n",
      "Epoch 28: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [29/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2852, Train Accuracy: 33.3918, Train Precision: 0.1043, Train Recall: 0.1474, Train F1: 0.0790, Train Best Threshold: 0.7251, Train correct predictions: 22854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5843  all labels size: (17201, 20)\n",
      "Val Loss: 1.2846, Val Accuracy: 33.9690, Val Precision: 0.1183, Val Recall: 0.1468, Val F1: 0.0737, Val correct predictions: 5843\n",
      "Epoch 29: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [30/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:17<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2811, Train Accuracy: 32.9242, Train Precision: 0.1076, Train Recall: 0.1774, Train F1: 0.0871, Train Best Threshold: 0.7215, Train correct predictions: 22534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6100  all labels size: (17201, 20)\n",
      "Val Loss: 1.2825, Val Accuracy: 35.4631, Val Precision: 0.1009, Val Recall: 0.1734, Val F1: 0.0870, Val correct predictions: 6100\n",
      "Epoch 30: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [31/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:14<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2805, Train Accuracy: 32.4099, Train Precision: 0.1062, Train Recall: 0.1718, Train F1: 0.0853, Train Best Threshold: 0.7250, Train correct predictions: 22182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6009  all labels size: (17201, 20)\n",
      "Val Loss: 1.2827, Val Accuracy: 34.9340, Val Precision: 0.0973, Val Recall: 0.1568, Val F1: 0.0773, Val correct predictions: 6009\n",
      "Epoch 31: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [32/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:14<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2799, Train Accuracy: 32.9944, Train Precision: 0.1068, Train Recall: 0.1455, Train F1: 0.0771, Train Best Threshold: 0.7272, Train correct predictions: 22582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:22<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5910  all labels size: (17201, 20)\n",
      "Val Loss: 1.2777, Val Accuracy: 34.3585, Val Precision: 0.1464, Val Recall: 0.1526, Val F1: 0.0759, Val correct predictions: 5910\n",
      "Epoch 32: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [33/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:17<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2794, Train Accuracy: 33.4341, Train Precision: 0.1113, Train Recall: 0.1632, Train F1: 0.0841, Train Best Threshold: 0.7249, Train correct predictions: 22883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6051  all labels size: (17201, 20)\n",
      "Val Loss: 1.2770, Val Accuracy: 35.1782, Val Precision: 0.1017, Val Recall: 0.1717, Val F1: 0.0772, Val correct predictions: 6051\n",
      "Epoch 33: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [34/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:14<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2780, Train Accuracy: 33.3392, Train Precision: 0.1118, Train Recall: 0.1537, Train F1: 0.0816, Train Best Threshold: 0.7268, Train correct predictions: 22818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6933  all labels size: (17201, 20)\n",
      "Val Loss: 1.2884, Val Accuracy: 40.3058, Val Precision: 0.1107, Val Recall: 0.1355, Val F1: 0.0741, Val correct predictions: 6933\n",
      "Epoch 34: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [35/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:16<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2786, Train Accuracy: 33.4005, Train Precision: 0.1057, Train Recall: 0.1712, Train F1: 0.0851, Train Best Threshold: 0.7260, Train correct predictions: 22860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5859  all labels size: (17201, 20)\n",
      "Val Loss: 1.2751, Val Accuracy: 34.0620, Val Precision: 0.1101, Val Recall: 0.1790, Val F1: 0.0836, Val correct predictions: 5859\n",
      "Epoch 35: Learning rate = 0.0007\n",
      "Model improved and saved.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [36/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2757, Train Accuracy: 34.1705, Train Precision: 0.1097, Train Recall: 0.1669, Train F1: 0.0833, Train Best Threshold: 0.7269, Train correct predictions: 23387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:22<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6092  all labels size: (17201, 20)\n",
      "Val Loss: 1.2788, Val Accuracy: 35.4165, Val Precision: 0.1007, Val Recall: 0.1600, Val F1: 0.0738, Val correct predictions: 6092\n",
      "Epoch 36: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [37/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2758, Train Accuracy: 34.1968, Train Precision: 0.1115, Train Recall: 0.1902, Train F1: 0.0903, Train Best Threshold: 0.7235, Train correct predictions: 23405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5801  all labels size: (17201, 20)\n",
      "Val Loss: 1.2827, Val Accuracy: 33.7248, Val Precision: 0.1136, Val Recall: 0.1595, Val F1: 0.0838, Val correct predictions: 5801\n",
      "Epoch 37: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [38/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2772, Train Accuracy: 33.7074, Train Precision: 0.1076, Train Recall: 0.1847, Train F1: 0.0892, Train Best Threshold: 0.7234, Train correct predictions: 23070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5970  all labels size: (17201, 20)\n",
      "Val Loss: 1.2765, Val Accuracy: 34.7073, Val Precision: 0.1268, Val Recall: 0.1980, Val F1: 0.0902, Val correct predictions: 5970\n",
      "Epoch 38: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [39/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:13<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2747, Train Accuracy: 33.3786, Train Precision: 0.1096, Train Recall: 0.1968, Train F1: 0.0882, Train Best Threshold: 0.7245, Train correct predictions: 22845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:24<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 6272  all labels size: (17201, 20)\n",
      "Val Loss: 1.2799, Val Accuracy: 36.4630, Val Precision: 0.1158, Val Recall: 0.1599, Val F1: 0.0813, Val correct predictions: 6272\n",
      "Epoch 39: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [40/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:18<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2750, Train Accuracy: 32.6203, Train Precision: 0.1131, Train Recall: 0.1775, Train F1: 0.0893, Train Best Threshold: 0.7260, Train correct predictions: 22326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 4752  all labels size: (17201, 20)\n",
      "Val Loss: 1.2812, Val Accuracy: 27.6263, Val Precision: 0.1351, Val Recall: 0.2101, Val F1: 0.0777, Val correct predictions: 4752\n",
      "Epoch 40: Learning rate = 0.0007\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [41/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:15<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2744, Train Accuracy: 33.8944, Train Precision: 0.1085, Train Recall: 0.2169, Train F1: 0.0977, Train Best Threshold: 0.7223, Train correct predictions: 23198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5439  all labels size: (17201, 20)\n",
      "Val Loss: 1.2772, Val Accuracy: 31.6203, Val Precision: 0.1114, Val Recall: 0.2285, Val F1: 0.0980, Val correct predictions: 5439\n",
      "Epoch 41: Learning rate = 0.00049\n",
      "Model did not improved\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch [42/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2139/2139 [05:17<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2710, Train Accuracy: 33.7746, Train Precision: 0.1114, Train Recall: 0.2119, Train F1: 0.0947, Train Best Threshold: 0.7244, Train correct predictions: 23116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 538/538 [01:23<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prediction: 17201  Correct preds: 5043  all labels size: (17201, 20)\n",
      "Val Loss: 1.2781, Val Accuracy: 29.3181, Val Precision: 0.1692, Val Recall: 0.2146, Val F1: 0.0798, Val correct predictions: 5043\n",
      "Epoch 42: Learning rate = 0.00049\n",
      "Model did not improved\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    \n",
    "    # Train the model for one epoch\n",
    "    train_loss, train_accuracy, train_precision, train_recall, train_f1, train_best_threshold, train_correct_predictions = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "      f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, \"\n",
    "      f\"Train F1: {train_f1:.4f}, Train Best Threshold: {train_best_threshold:.4f}, \"\n",
    "      f\"Train correct predictions: {train_correct_predictions}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Validate the model after training\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1, val_correct_predictions = validate(model, val_loader, criterion, device, train_best_threshold)\n",
    "\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, \"\n",
    "      f\"Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, \"\n",
    "      f\"Val F1: {val_f1:.4f}, Val correct predictions: {val_correct_predictions}\")\n",
    "\n",
    "\n",
    "    # Step the Lookahead optimizer\n",
    "    lookahead_optimizer.step()\n",
    "    # Step the ReduceLROnPlateau scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Learning rate = {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "    num_epochs_runned += 1\n",
    "\n",
    "    # Validation loss has improved\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        epochs_no_improve = 0  # Reset the counter if we see improvement\n",
    "        \n",
    "        # Save the model checkpoint\n",
    "        torch.save(model.state_dict(), \"singular_weighted_classes.pth\")\n",
    "        print(\"Model improved and saved.\")\n",
    "    else:\n",
    "        print(\"Model did not improved\")\n",
    "        epochs_no_improve += 1  # Increment counter if no improvement\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve == patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd155298-30d9-4bfa-b980-1abfe2318399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label names for better readability in the report\n",
    "label_names = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', \n",
    "    'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule', \n",
    "    'Pleural Thickening', 'Pneumonia', 'Pneumothorax', 'Pneumoperitoneum', \n",
    "    'Pneumomediastinum', 'Subcutaneous Emphysema', 'Tortuous Aorta', \n",
    "    'Calcification of the Aorta', 'No Finding'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99e00203e15923f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:53:15.350469Z",
     "start_time": "2024-11-10T14:52:46.256405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 659/659 [01:37<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding Best Thresholds for Each Class:\n",
      "Class 'Atelectasis': Best Threshold = 0.5944, Max F1-Score = 0.2605\n",
      "Class 'Cardiomegaly': Best Threshold = 0.6693, Max F1-Score = 0.1153\n",
      "Class 'Consolidation': Best Threshold = 0.7205, Max F1-Score = 0.1982\n",
      "Class 'Edema': Best Threshold = 0.7311, Max F1-Score = 0.1351\n",
      "Class 'Effusion': Best Threshold = 0.6934, Max F1-Score = 0.3803\n",
      "Class 'Emphysema': Best Threshold = 0.6550, Max F1-Score = 0.0975\n",
      "Class 'Fibrosis': Best Threshold = 0.6631, Max F1-Score = 0.0837\n",
      "Class 'Hernia': Best Threshold = 0.7311, Max F1-Score = 0.0519\n",
      "Class 'Infiltration': Best Threshold = 0.6226, Max F1-Score = 0.4316\n",
      "Class 'Mass': Best Threshold = 0.6338, Max F1-Score = 0.1384\n",
      "Class 'Nodule': Best Threshold = 0.5079, Max F1-Score = 0.1457\n",
      "Class 'Pleural Thickening': Best Threshold = 0.6391, Max F1-Score = 0.1145\n",
      "Class 'Pneumonia': Best Threshold = 0.7266, Max F1-Score = 0.0737\n",
      "Class 'Pneumothorax': Best Threshold = 0.5538, Max F1-Score = 0.2274\n",
      "Class 'Pneumoperitoneum': Best Threshold = 0.7299, Max F1-Score = 0.0165\n",
      "Class 'Pneumomediastinum': Best Threshold = 0.7311, Max F1-Score = 0.0364\n",
      "Class 'Subcutaneous Emphysema': Best Threshold = 0.5723, Max F1-Score = 0.0772\n",
      "Class 'Tortuous Aorta': Best Threshold = 0.7307, Max F1-Score = 0.0380\n",
      "Class 'Calcification of the Aorta': Best Threshold = 0.7310, Max F1-Score = 0.0414\n",
      "Class 'No Finding': Best Threshold = 0.5232, Max F1-Score = 0.5560\n",
      "\n",
      "Classification Report with Best Thresholds:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "               Atelectasis       0.16      0.68      0.26      2700\n",
      "              Cardiomegaly       0.07      0.44      0.12       868\n",
      "             Consolidation       0.12      0.57      0.20      1497\n",
      "                     Edema       0.09      0.30      0.13       751\n",
      "                  Effusion       0.26      0.70      0.38      3735\n",
      "                 Emphysema       0.06      0.22      0.10       917\n",
      "                  Fibrosis       0.05      0.19      0.08       365\n",
      "                    Hernia       0.03      0.13      0.05        62\n",
      "              Infiltration       0.32      0.65      0.43      5159\n",
      "                      Mass       0.09      0.31      0.14      1329\n",
      "                    Nodule       0.09      0.46      0.15      1305\n",
      "        Pleural Thickening       0.07      0.34      0.11       902\n",
      "                 Pneumonia       0.05      0.18      0.07       452\n",
      "              Pneumothorax       0.14      0.54      0.23      2106\n",
      "          Pneumoperitoneum       0.01      0.01      0.01        69\n",
      "         Pneumomediastinum       0.02      0.10      0.03       143\n",
      "    Subcutaneous Emphysema       0.04      0.50      0.08       813\n",
      "            Tortuous Aorta       0.02      0.12      0.03        95\n",
      "Calcification of the Aorta       0.02      0.09      0.03        55\n",
      "                No Finding       0.45      0.73      0.56      8015\n",
      "\n",
      "                 micro avg       0.18      0.59      0.27     31338\n",
      "                 macro avg       0.11      0.36      0.16     31338\n",
      "              weighted avg       0.25      0.59      0.33     31338\n",
      "               samples avg       0.25      0.61      0.32     31338\n",
      "\n",
      "\n",
      "Classification Report with Default Threshold (0.5):\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "               Atelectasis       0.14      0.96      0.24      2700\n",
      "              Cardiomegaly       0.04      0.99      0.08       868\n",
      "             Consolidation       0.08      0.94      0.16      1497\n",
      "                     Edema       0.05      0.96      0.09       751\n",
      "                  Effusion       0.20      0.97      0.33      3735\n",
      "                 Emphysema       0.05      0.96      0.09       917\n",
      "                  Fibrosis       0.02      0.95      0.04       365\n",
      "                    Hernia       0.01      0.74      0.01        62\n",
      "              Infiltration       0.25      0.99      0.40      5159\n",
      "                      Mass       0.07      0.96      0.12      1329\n",
      "                    Nodule       0.06      0.96      0.12      1305\n",
      "        Pleural Thickening       0.05      0.84      0.09       902\n",
      "                 Pneumonia       0.02      0.88      0.05       452\n",
      "              Pneumothorax       0.10      0.98      0.19      2106\n",
      "          Pneumoperitoneum       0.00      0.75      0.01        69\n",
      "         Pneumomediastinum       0.01      0.41      0.02       143\n",
      "    Subcutaneous Emphysema       0.04      0.85      0.07       813\n",
      "            Tortuous Aorta       0.01      0.54      0.01        95\n",
      "Calcification of the Aorta       0.00      0.76      0.01        55\n",
      "                No Finding       0.38      1.00      0.55      8015\n",
      "\n",
      "                 micro avg       0.09      0.96      0.17     31338\n",
      "                 macro avg       0.08      0.87      0.13     31338\n",
      "              weighted avg       0.20      0.96      0.31     31338\n",
      "               samples avg       0.11      0.97      0.18     31338\n",
      "\n",
      "Test Loss: 1.8057\n"
     ]
    }
   ],
   "source": [
    "# Now, evaluate the model on the test set\n",
    "model.load_state_dict(torch.load(\"singular_weighted_classes.pth\", weights_only=True))\n",
    "\n",
    "test_loss, test_report_best, test_report_default, test_best_thresholds = test(model, test_loader, criterion, device, label_names, train_best_threshold)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5765d547daa4988b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T15:05:50.622585Z",
     "start_time": "2024-11-10T15:05:50.415197Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs_runned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n\u001B[0;32m----> 2\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[38;5;28mrange\u001B[39m(\u001B[43mnum_epochs_runned\u001B[49m), train_losses, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain Loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[38;5;28mrange\u001B[39m(num_epochs_runned), val_losses, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation Loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'num_epochs_runned' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(num_epochs_runned), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(num_epochs_runned), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss singular weighted classes')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(range(num_epochs_runned), train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(range(num_epochs_runned), val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Save the results in a DataFrame for reporting\n",
    "results_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs_runned + 1),\n",
    "    'Train Loss': train_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Val Loss': val_losses,\n",
    "    'Val Accuracy': val_accuracies\n",
    "})\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c6e8c-1485-4f00-9fcc-167a6f2a043a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
